# open-deepwiki configuration
# Loaded by the FastAPI app (and future indexer) via `OPEN_DEEPWIKI_CONFIG` or this default filename.

debug_level: INFO

# FastAPI / Uvicorn settings (used when starting via `./venv/bin/python app.py`)
api_port: 8000

# JavaDoc generation
# If an existing JavaDoc has fewer meaningful content lines than this value,
# it will be replaced ("improved").
javadoc_min_meaningful_lines: 3

# Chroma
# Disable anonymized telemetry (see https://docs.trychroma.com/telemetry)
chroma_anonymized_telemetry: false

# Root directory to scan/index for Java code.
# Example: /abs/path/to/repo or ./fixtures
java_codebase_dir: ./fixtures
# LLM (embeddings + chat)
# Ces champs alimentent automatiquement des variables d'env compatibles OpenAI/LangChain :
# - llm_api_key       -> OPENAI_API_KEY
# - llm_api_base      -> OPENAI_API_BASE (optionnel)
# - embeddings_api_base -> OPENAI_EMBEDDING_API_BASE (optionnel; override pour embeddings)
# - chat_api_base       -> OPENAI_CHAT_API_BASE (optionnel; override pour chat)
# - embeddings_model  -> OPENAI_EMBEDDING_MODEL
# - chat_model        -> OPENAI_CHAT_MODEL
#
# IMPORTANT (mode strict): pas de fallback implicite.
# - Si tu utilises un serveur custom, renseigne explicitement `embeddings_api_base` et `chat_api_base`.
# - Si tu veux la même URL pour les deux, mets la même valeur dans les deux champs.
#
# Exemple:
# embeddings_model: text-embedding-3-large
# chat_model: gpt-4o-mini
# llm_api_base: https://your-internal-api.example.com/v1
# embeddings_api_base: https://your-embeddings-gateway.example.com/v1
# chat_api_base: https://your-llm-gateway.example.com/v1
# llm_api_key: sk-...

embeddings_model: text-embedding-3-large
chat_model: gpt-4o-mini
llm_api_base: https://api.openai.com/v1
embeddings_api_base: https://api.openai.com/v1
chat_api_base: https://api.openai.com/v1
llm_api_key: null # mets la clé ici, ou préfère l’env `OPENAI_API_KEY`

# SSL / TLS
# Optional: path to a PEM file containing root CA certificates for outbound HTTPS.
# Useful behind corporate proxies / custom PKI.
# If set, it is applied to: SSL_CERT_FILE, REQUESTS_CA_BUNDLE, CURL_CA_BUNDLE
ssl_ca_file: null

# Outbound proxies (downloads + API calls)
# These map to standard env vars: HTTP_PROXY/HTTPS_PROXY/NO_PROXY (also lower-case variants).
# Example:
# http_proxy: http://proxy.mycorp.local:3128
# https_proxy: http://proxy.mycorp.local:3128
# no_proxy: 127.0.0.1,localhost,.mycorp.local
http_proxy: null
https_proxy: null
no_proxy: null

# Optional: tiktoken cache directory (helps if encodings must be downloaded through proxy)
# Maps to: TIKTOKEN_CACHE_DIR
tiktoken_cache_dir: null

# Optional: prefetch tiktoken encodings at startup (forces download/caching)
# If enabled and no encodings are provided, defaults to: ["cl100k_base"]
# Example:
# tiktoken_prefetch: true
# tiktoken_prefetch_encodings:
#   - cl100k_base
#   - o200k_base
tiktoken_prefetch: false
tiktoken_prefetch_encodings: null
