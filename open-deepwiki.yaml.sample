# open-deepwiki configuration
# Loaded by the FastAPI app (and future indexer) via `OPEN_DEEPWIKI_CONFIG` or this default filename.

debug_level: INFO

# FastAPI / Uvicorn settings (used when starting via `./venv/bin/python app.py`)
api_port: 8000

# CORS
# If true, enables permissive CORS headers for browser clients (dev-friendly).
# If false, no CORS middleware is installed.
cors_enabled: false

# JavaDoc generation
# If an existing JavaDoc has fewer meaningful content lines than this value,
# it will be replaced ("improved").
javadoc_min_meaningful_lines: 3

# Chroma
# Disable anonymized telemetry (see https://docs.trychroma.com/telemetry)
chroma_anonymized_telemetry: false

# Root directory to scan/index for Java code.
# Example: /abs/path/to/repo or ./fixtures
java_codebase_dir: ./fixtures

# Optional project scope name.
# When set, indexed docs include metadata.project=<project_name> and API calls can
# omit the `project` parameter to use this default.
project_name: null

# Documentation site output
# Base directory where generated documentation artifacts are written.
# The /index-directory endpoint will write:
# - <docs_output_dir>/PROJECT_OVERVIEW.md
# - <docs_output_dir>/docs/index.md
# - <docs_output_dir>/docs/features/*.md
docs_output_dir: OUTPUT

# Feature docs generation tuning
docs_feature_batch_size: 10

# Optional: index one heuristic summary document per Java file.
# Helps RAG answer file-level questions without reading every method.
index_file_summaries: false

# Optional: exclude test-related Java sources during indexing.
# If true (default), files located under a directory named "test" (case-insensitive)
# like src/test/java/** are ignored.
index_exclude_tests: true

# /ask persistence is handled by the LangGraph checkpointer (see bottom of file).
# LLM (embeddings + chat)
# Ces champs alimentent automatiquement des variables d'env compatibles OpenAI/LangChain :
# - llm_api_key       -> OPENAI_API_KEY
# - llm_api_base      -> OPENAI_API_BASE + (par défaut) OPENAI_EMBEDDING_API_BASE + OPENAI_CHAT_API_BASE
# - embeddings_model  -> OPENAI_EMBEDDING_MODEL
# - chat_model        -> OPENAI_CHAT_MODEL
#
# IMPORTANT:
# - Si tu veux la même URL pour embeddings + chat, renseigne uniquement `llm_api_base`.
# - Si tu as des gateways séparés, configure directement les env vars OPENAI_EMBEDDING_API_BASE / OPENAI_CHAT_API_BASE.
#
# Exemple:
# embeddings_model: text-embedding-3-large
# chat_model: gpt-4o-mini
# llm_api_base: https://your-internal-api.example.com/v1
# llm_api_key: sk-...

embeddings_model: text-embedding-3-large
chat_model: gpt-4o-mini
llm_api_base: https://api.openai.com/v1
llm_api_key: null # mets la clé ici, ou préfère l’env `OPENAI_API_KEY`

# Embeddings compatibility
# If true, the embeddings client may send token-id arrays for long inputs.
# Many OpenAI-compatible embedding servers only accept string inputs.
# Default is false (string inputs).
embeddings_check_ctx_length: false

# Embeddings input size protection
# If set, embedding inputs are truncated to at most this many tokens before being sent.
# This avoids provider-side "input too long" errors.
# Note: token counting uses tiktoken.
# Example:
# embeddings_max_input_tokens: 7500
embeddings_max_input_tokens: null

# Optional: force a specific tiktoken encoding when applying the limit.
# If null, the app will try to infer an encoding from embeddings_model and fall back to cl100k_base.
# Example values: cl100k_base, o200k_base
embeddings_token_encoding: null

# SSL / TLS
# Optional: path to a PEM file containing root CA certificates for outbound HTTPS.
# Useful behind corporate proxies / custom PKI.
# If set, it is applied to: SSL_CERT_FILE, REQUESTS_CA_BUNDLE, CURL_CA_BUNDLE
ssl_ca_file: null

# Outbound proxies (downloads + API calls)
# These map to standard env vars: HTTP_PROXY/HTTPS_PROXY/NO_PROXY (also lower-case variants).
# Example:
# http_proxy: http://proxy.mycorp.local:3128
# https_proxy: http://proxy.mycorp.local:3128
# no_proxy: 127.0.0.1,localhost,.mycorp.local
http_proxy: null
https_proxy: null
no_proxy: null

# Optional: tiktoken cache directory (helps if encodings must be downloaded through proxy)
# Maps to: TIKTOKEN_CACHE_DIR
# Note: tiktoken caches by sha1(url), not by original filename.
tiktoken_cache_dir: null

# Optional: prefetch tiktoken encodings at startup (forces download/caching)
# If enabled and no encodings are provided, defaults to: ["cl100k_base"]
# Example:
# tiktoken_prefetch: true
# tiktoken_prefetch_encodings:
#   - cl100k_base
#   - o200k_base
tiktoken_prefetch: false
tiktoken_prefetch_encodings: null

# Agent persistence (LangGraph checkpointer)
# Persists agent state (including message history + tool call traces) across restarts.
# Supported backends: sqlite
checkpointer_backend: sqlite
checkpointer_sqlite_path: ./checkpoints.sqlite3

# Project graph persistence
# Stores a lightweight project graph (files/methods + best-effort call edges) used to
# generate a "project overview" document and power graph tools.
project_graph_sqlite_path: ./project_graph.sqlite3
